# –õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ3

# –ó–∞–¥–∞–Ω–∏–µ A
### ¬†–ó–∞–ø—É—Å–∫ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞: `python3 -m src.lab03.A`
```python
from src.lib.text import normalize, tokenize, top_n, count_freq


print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞", yo2e=True))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))


print(top_n(count_freq(["a", "b", "a", "c", "b", "a"]), n=2))
print(top_n(count_freq(["bb", "aa", "bb", "aa", "cc"]), n=2))
```
![–í—ã–≤–æ–¥ –∑–∞–¥–∞–Ω–∏–µ A](./images/lab03/ex_A.png)

# –ó–∞–¥–∞–Ω–∏–µ B
### ¬†–ó–∞–ø—É—Å–∫ –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞: `python3 -m src.lab03.B < src/lab03/input.txt`
```python
import sys
from src.lib.text import count_freq, top_n, normalize, tokenize

text = sys.stdin.read()

tokens = tokenize(text=normalize(text=text))
top = top_n(count_freq(tokens))

print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(set(tokens))}")
print("–¢–æ–ø-5:")
for w, c in top:
    print(f"{w}:{c}")
```
![–í—ã–≤–æ–¥ –∑–∞–¥–∞–Ω–∏–µ B](./images/lab03/ex_B.png)
